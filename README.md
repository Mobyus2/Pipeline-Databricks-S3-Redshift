# ğŸ”¥ PIPELINE DU SUIVI DES VENTES
## *Du Streaming de DonnÃ©es Ã  la Visualisation dans Databricks*

<div align="center">

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                  â•‘
â•‘      Amazon S3  â”€â”€â–º  Databricks  â”€â”€â–º  Bronze â–º Silver â–º Gold    â•‘
â•‘                                            â”‚                     â•‘
â•‘                              Genie AI â—„â”€â”€â”€â”€â”˜â”€â”€â”€â”€â–º BI Dashboard  â•‘
â•‘                                                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Apache Spark](https://img.shields.io/badge/Apache%20Spark-E25A1C?style=for-the-badge&logo=apachespark&logoColor=white)
![Databricks](https://img.shields.io/badge/Databricks-FF3621?style=for-the-badge&logo=databricks&logoColor=white)
![Amazon S3](https://img.shields.io/badge/Amazon%20S3-569A31?style=for-the-badge&logo=amazons3&logoColor=white)
![SQL](https://img.shields.io/badge/SQL-4479A1?style=for-the-badge&logo=postgresql&logoColor=white)

**Auteur :** Francois Louis Marie NTONGA Â· Data Engineer Junior  
ğŸ“§ francoislouismarie.contact@gmail.com Â· [LinkedIn](www.linkedin.com/in/francois-louis-marie-ntonga-7b982329b)

</div>

---

## ğŸ“Œ Vue d'ensemble

> **Contexte :** Une entreprise FMCG (Fast-Moving Consumer Goods) vient d'acquÃ©rir une seconde sociÃ©tÃ©. Les donnÃ©es de ventes des deux entitÃ©s sont Ã©parpillÃ©es, hÃ©tÃ©rogÃ¨nes, inexploitÃ©es. **Mission : les unifier, les transformer, les rendre dÃ©cisionnelles â€” de A Ã  Z.**

Ce projet conÃ§oit et industrialise un **pipeline ETL bout-Ã -bout** en s'appuyant sur Databricks comme socle central. Le rÃ©sultat ? Une plateforme Data Engineering complÃ¨te, automatisÃ©e, scalable â€” oÃ¹ la donnÃ©e brute devient un actif stratÃ©gique exploitable par les mÃ©tiers en quelques clics.

---

## ğŸ—ï¸ Architecture Globale

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SOURCE DE DONNÃ‰ES                           â”‚
â”‚                                                                     â”‚
â”‚   ğŸ“ customers/    ğŸ“ orders/    ğŸ“ products/    ğŸ“ gross_price/   â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                        Amazon S3 (Data Lake)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    IAM Role + External Location
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ğŸ§± DATABRICKS LAKEHOUSE                        â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   BRONZE    â”‚â”€â”€â”€â–ºâ”‚   SILVER    â”‚â”€â”€â”€â–ºâ”‚        GOLD          â”‚   â”‚
â”‚  â”‚  (Raw Data) â”‚    â”‚  (Cleaned)  â”‚    â”‚ (Star Schema / BI)   â”‚   â”‚
â”‚  â”‚             â”‚    â”‚             â”‚    â”‚                      â”‚   â”‚
â”‚  â”‚ â€¢ customers â”‚    â”‚ â€¢ customers â”‚    â”‚ â€¢ dim_customers      â”‚   â”‚
â”‚  â”‚ â€¢ orders    â”‚    â”‚ â€¢ orders    â”‚    â”‚ â€¢ dim_products       â”‚   â”‚
â”‚  â”‚ â€¢ products  â”‚    â”‚ â€¢ products  â”‚    â”‚ â€¢ dim_date           â”‚   â”‚
â”‚  â”‚ â€¢ gross_    â”‚    â”‚ â€¢ gross_    â”‚    â”‚ â€¢ dim_gross_price    â”‚   â”‚
â”‚  â”‚   price     â”‚    â”‚   price     â”‚    â”‚ â€¢ fact_orders        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â€¢ vw_fact_orders_   â”‚   â”‚
â”‚                                        â”‚   enriched (VIEW)   â”‚   â”‚
â”‚                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                 â”‚                   â”‚
â”‚                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚                              â”‚                              â”‚      â”‚
â”‚                              â–¼                              â–¼      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                    â”‚   ğŸ¤– Genie AI    â”‚        â”‚ ğŸ“Š BI Dashboardâ”‚  â”‚
â”‚                    â”‚ (NL â†’ SQL auto)  â”‚        â”‚ (Sales Insightsâ”‚  â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Stack Technique

| Composant | Technologie | RÃ´le |
|---|---|---|
| ğŸ—„ï¸ **Data Lake** | Amazon S3 | Stockage des donnÃ©es sources (CSV) |
| âš™ï¸ **Compute** | Apache Spark | Traitement distribuÃ© & scalable |
| ğŸ§  **Plateforme** | Databricks | Orchestration, notebooks, jobs |
| ğŸ’¾ **Format** | Delta Lake | Tables versionnÃ©es & ACID |
| ğŸ—£ï¸ **Langage** | Python & SQL | Transformation & modÃ©lisation |
| ğŸ¤– **IA** | Databricks Genie | Interrogation en langage naturel |
| ğŸ“Š **Viz** | Databricks Dashboard | Restitution BI interactive |
| ğŸ” **Auth** | AWS IAM Role | Connexion sÃ©curisÃ©e S3 â†” Databricks |

---

## ğŸ“‚ Structure du Projet

```
fmcg-databricks-pipeline/
â”‚
â”œâ”€â”€ ğŸ“ 0_data/                                     # DonnÃ©es sources (â†’ Amazon S3)
â”‚   â”œâ”€â”€ ğŸ“ 1_parent_company/                       # SociÃ©tÃ© principale (acquÃ©rante)
â”‚   â”‚   â”œâ”€â”€ ğŸ“ full_load/                          # Chargement initial complet
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_customers.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_gross_price.csv
â”‚   â”‚   â”‚   â”œâ”€â”€ dim_products.csv
â”‚   â”‚   â”‚   â””â”€â”€ fact_orders.csv
â”‚   â”‚   â””â”€â”€ ğŸ“ incremental_load/                   # Mises Ã  jour quotidiennes
â”‚   â”‚       â”œâ”€â”€ fact_orders.csv
â”‚   â”‚       â””â”€â”€ incremental_data_parent_company_query.txt
â”‚   â””â”€â”€ ğŸ“ 2_child_company/                        # SociÃ©tÃ© acquise
â”‚
â”œâ”€â”€ ğŸ“ 1_codes/                                    # Notebooks Databricks
â”‚   â”œâ”€â”€ ğŸ“ 1_setup/                                # Initialisation de l'environnement
â”‚   â”‚   â”œâ”€â”€ dim_date_table_creation.ipynb          # GÃ©nÃ©ration de la dimension date
â”‚   â”‚   â”œâ”€â”€ setup_catalog.ipynb                    # CrÃ©ation catalog + schÃ©mas Bronze/Silver/Gold
â”‚   â”‚   â””â”€â”€ utilities.ipynb                        # Fonctions utilitaires partagÃ©es
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ 2_dimension_data_processing/            # Traitement des dimensions (Silver â†’ Gold)
â”‚   â”‚   â”œâ”€â”€ 1_customers_data_processing.ipynb      # Pipeline clients
â”‚   â”‚   â”œâ”€â”€ 2_products_data_processing.ipynb       # Pipeline produits
â”‚   â”‚   â””â”€â”€ 3_pricing_data_processing.ipynb        # Pipeline prix
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ 3_fact_data_processing/                 # Traitement de la table de faits
â”‚       â”œâ”€â”€ 1_full_load_fact.ipynb                 # Chargement historique complet
â”‚       â””â”€â”€ 2_incremental_load_fact.ipynb          # Chargement incrÃ©mental quotidien
â”‚
â”œâ”€â”€ ğŸ“ 2_dashboarding/                             # Restitution & visualisation
â”‚   â”œâ”€â”€ denormalise_table_query_fmcg.txt           # RequÃªte SQL de la vue enrichie
â”‚   â””â”€â”€ fmcg_dashboard.pdf                        # Export du dashboard Sales Insights
â”‚
â”œâ”€â”€ ğŸ“ resources/                                  # Documentation & design
â”‚   â”œâ”€â”€ databricks_project.excalidraw             # SchÃ©ma d'architecture (Ã©ditable)
â”‚   â””â”€â”€ project_architecture.png                  # SchÃ©ma d'architecture (export)
â”‚
â”œâ”€â”€ Projet_DataBricks_Data_Ops.pdf                # Documentation complÃ¨te du projet
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline en 15 Ã‰tapes

### **PHASE 1 â€” Infrastructure & Connexion**

#### Ã‰tape 1 Â· CrÃ©ation du bucket Amazon S3
Mise en place du Data Lake S3 (`sportbar-bp`) avec une organisation par domaine mÃ©tier :
```
s3://sportbar-bp/
    â”œâ”€â”€ customers/
    â”œâ”€â”€ orders/
    â”œâ”€â”€ products/
    â””â”€â”€ gross_price/
```

#### Ã‰tape 2 Â· Liaison sÃ©curisÃ©e Databricks â†” S3
Configuration d'une **External Location** via un rÃ´le IAM dÃ©diÃ© â€” aucune clÃ© exposÃ©e, accÃ¨s en lecture/Ã©criture natif.

#### Ã‰tape 3 Â· Validation de la connexion
Test complet des permissions : Read âœ… Â· Write âœ… Â· Delete âœ… Â· Assume Role âœ… Â· External ID âœ…

#### Ã‰tape 4 Â· CrÃ©ation du Catalog & des schÃ©mas
```sql
CREATE CATALOG IF NOT EXISTS fmcg;
USE CATALOG fmcg;

CREATE SCHEMA IF NOT EXISTS fmcg.bronze;
CREATE SCHEMA IF NOT EXISTS fmcg.silver;
CREATE SCHEMA IF NOT EXISTS fmcg.gold;
```

---

### **PHASE 2 â€” Medallion Architecture**

#### Ã‰tape 5 Â· Lecture des donnÃ©es depuis S3 avec Spark
```python
basepath = f's3://sportbar-bp/{data_source}/*.csv'
df = spark.read.format("csv").load(basepath)
display(df.limit(10))
```

#### Ã‰tape 6 Â· Couche Bronze â€” Ingestion brute
Les donnÃ©es CSV sont chargÃ©es **telles quelles** dans Databricks â€” structure d'origine prÃ©servÃ©e, sans transformation mÃ©tier.

```
fmcg.bronze
    â”œâ”€â”€ customers    (raw)
    â”œâ”€â”€ orders       (raw)
    â”œâ”€â”€ products     (raw)
    â””â”€â”€ gross_price  (raw)
```

#### Ã‰tape 7 Â· Couche Silver â€” Nettoyage & structuration
Filtrage, typage correct, normalisation et enrichissement des donnÃ©es Bronze.

```
fmcg.silver
    â”œâ”€â”€ customers    (cleaned & typed)
    â”œâ”€â”€ orders       (cleaned & typed)
    â”œâ”€â”€ products     (cleaned & typed)
    â””â”€â”€ gross_price  (cleaned & typed)
```

#### Ã‰tape 8 Â· Couche Gold â€” ModÃ¨le analytique en Ã©toile

```
fmcg.gold
    â”œâ”€â”€ dim_customers          â† Dimension clients
    â”œâ”€â”€ dim_products           â† Dimension produits
    â”œâ”€â”€ dim_date               â† Dimension temporelle
    â”œâ”€â”€ dim_gross_price        â† Dimension prix
    â”œâ”€â”€ fact_orders            â† Table de faits centrale
    â”œâ”€â”€ sb_dim_customers       â† (sociÃ©tÃ© B)
    â”œâ”€â”€ sb_dim_products        â† (sociÃ©tÃ© B)
    â”œâ”€â”€ sb_dim_gross_price     â† (sociÃ©tÃ© B)
    â”œâ”€â”€ sb_fact_orders         â† (sociÃ©tÃ© B)
    â””â”€â”€ vw_fact_orders_enriched â† Vue analytique unifiÃ©e
```

**Star Schema :**
```
         dim_customers
               â”‚
dim_date â”€â”€â”€â”€ fact_orders â”€â”€â”€â”€ dim_products
               â”‚
         dim_gross_price
```

#### Ã‰tape 9 Â· Vue analytique enrichie `vw_fact_orders_enriched`
```sql
CREATE OR REPLACE VIEW fmcg.gold.vw_fact_orders_enriched AS
SELECT
    fo.date,
    fo.product_code,
    fo.customer_code,
    -- Date attributes
    dd.date_key, dd.year, dd.month_name, dd.month_short_name, dd.quarter,
    dd.year_quarter,
    -- Customer attributes
    ...
FROM fmcg.gold.fact_orders fo
JOIN fmcg.gold.dim_date dd    ON fo.date = dd.date_key
JOIN fmcg.gold.dim_customers dc ON fo.customer_code = dc.customer_code
JOIN fmcg.gold.dim_products dp  ON fo.product_code = dp.product_code
```

---

### **PHASE 3 â€” Orchestration & Automatisation**

#### Ã‰tape 10 Â· Pipeline Databricks Jobs
Un workflow complet orchestre l'ensemble des traitements avec gestion des dÃ©pendances :

```
dim_processing_customer  â”€â”€â”
dim_processing_products  â”€â”€â”¼â”€â”€â–º fact_processing_orders
dim_processing_prices    â”€â”€â”˜
```
> Les dimensions sont calculÃ©es **en parallÃ¨le** ; la table de faits attend leur complÃ©tion.

#### Ã‰tape 11 Â· Test manuel â€” `Run now`
DÃ©clenchement manuel pour valider les dÃ©pendances et les performances.

#### Ã‰tape 12 Â· Monitoring en temps rÃ©el
Suivi visuel de chaque tÃ¢che : Ã©tat, durÃ©e, erreurs, logs â€” directement dans l'interface Databricks Jobs.

#### Ã‰tape 13 Â· Planification automatique (Trigger)
```
Schedule: Every Day at 21:00 UTC
Trigger Status: â— Active
```
Le pipeline s'exÃ©cute dÃ©sormais **de maniÃ¨re autonome**, sans intervention humaine.

---

### **PHASE 4 â€” Consommation & Visualisation**

#### Ã‰tape 14 Â· Genie IA â€” Interrogation en langage naturel
Un espace **Databricks Genie** connectÃ© Ã  `vw_fact_orders_enriched` permet d'explorer les donnÃ©es sans Ã©crire une ligne de SQL :

```
ğŸ’¬ "Show me total revenues by quarter"
ğŸ’¬ "What are the top 5 customers by sold quantity?"
ğŸ’¬ "What is the monthly total sales amount over time?"
```

â†’ Genie gÃ©nÃ¨re automatiquement les requÃªtes SQL + visualisations (bar charts, histogrammes, courbes temporelles).

#### Ã‰tape 15 Â· Dashboard BI interactif â€” Sales Insights
Dashboard final dÃ©ployÃ© dans Databricks avec filtres dynamiques (AnnÃ©e / Trimestre / Mois / Canal / CatÃ©gorie).

---

## ğŸ“Š KPIs du Dashboard

### Performance Globale
| KPI | Valeur |
|---|---|
| ğŸ’° Total Revenue | **105.34 B** |
| ğŸ“¦ Total Quantity Sold | **34.13 M unitÃ©s** |
| ğŸ‘¥ Clients uniques | **54** |
| ğŸ’² Prix moyen de vente | **4 043.16** |

### Analyses disponibles
- ğŸ“ˆ **Monthly Revenue Trend** â€” SaisonnalitÃ© & pics (Q4 trÃ¨s fort)
- ğŸ† **Top Products by Revenue** â€” Focus marketing & gestion stocks
- ğŸ¥‡ **Top Customers by Revenue** â€” FitnessWorld Â· FastTrack Sports Â· Fitness Mania
- ğŸ° **Revenue Share by Channel** â€” Retailer 78% Â· Direct 20%
- ğŸ“ **Product Price vs Quantity** â€” Scatter plot prix/volume
- ğŸ¯ **Top Variant by Revenue** â€” Large Â· 9kg Â· Curl Bar Â· Medium Â· Youth...

---

## ğŸ“ˆ RÃ©sultats & Valeur MÃ©tier

```
âœ… Pipeline ETL automatisÃ© et planifiÃ© quotidiennement
âœ… Architecture Medallion Bronze â†’ Silver â†’ Gold opÃ©rationnelle
âœ… Star Schema analytique pour requÃªtes SQL performantes
âœ… DonnÃ©es de 2 sociÃ©tÃ©s consolidÃ©es en un modÃ¨le unifiÃ©
âœ… Exploration IA en langage naturel via Genie
âœ… Dashboard interactif accessible aux non-techniciens
âœ… Pipeline exÃ©cutÃ© avec succÃ¨s (statut OK, toutes tÃ¢ches âœ“)
```

### DÃ©cisions mÃ©tiers rendues possibles
- Suivi et pilotage du chiffre d'affaires en temps rÃ©el
- Analyse des performances par client, produit, canal et pÃ©riode
- Identification des produits et clients les plus rentables
- Analyse des tendances et saisonnalitÃ© des ventes
- Optimisation des stratÃ©gies commerciales et de distribution

---

## âš™ï¸ PrÃ©-requis & DÃ©ploiement

### PrÃ©-requis
- Compte Databricks (avec Unity Catalog activÃ©)
- Bucket Amazon S3 configurÃ©
- RÃ´le IAM avec permissions S3 (Read / Write / Delete / AssumeRole)
- Cluster Spark (ou Serverless Compute)

### DÃ©marrage rapide

```bash
# 1. Cloner le repo
git clone https://github.com/francoislouismarie/fmcg-databricks-pipeline.git
```

**2. Uploader les donnÃ©es dans S3**
```
s3://sportbar-bp/
    â”œâ”€â”€ customers/     â† 0_data/1_parent_company/full_load/dim_customers.csv
    â”œâ”€â”€ products/      â† 0_data/1_parent_company/full_load/dim_products.csv
    â”œâ”€â”€ gross_price/   â† 0_data/1_parent_company/full_load/dim_gross_price.csv
    â””â”€â”€ orders/        â† 0_data/1_parent_company/full_load/fact_orders.csv
```

**3. Configurer la connexion S3 â†” Databricks**
```
Databricks > Catalog > External Locations > Add
â†’ CrÃ©er un Storage Credential (IAM Role)
â†’ CrÃ©er une External Location pointant vers s3://sportbar-bp/
â†’ Tester la connexion (Read / Write / Delete / AssumeRole âœ…)
```

**4. ExÃ©cuter les notebooks dans l'ordre**
```
ğŸ“ 1_codes/1_setup/
    1. setup_catalog.ipynb              â†’ CrÃ©e le catalog fmcg + schÃ©mas Bronze/Silver/Gold
    2. utilities.ipynb                  â†’ Charge les fonctions utilitaires partagÃ©es
    3. dim_date_table_creation.ipynb    â†’ GÃ©nÃ¨re la dimension date (Gold)

ğŸ“ 1_codes/2_dimension_data_processing/
    4. 1_customers_data_processing.ipynb    â†’ Bronze â†’ Silver â†’ Gold (dim_customers)
    5. 2_products_data_processing.ipynb     â†’ Bronze â†’ Silver â†’ Gold (dim_products)
    6. 3_pricing_data_processing.ipynb      â†’ Bronze â†’ Silver â†’ Gold (dim_gross_price)

ğŸ“ 1_codes/3_fact_data_processing/
    7. 1_full_load_fact.ipynb           â†’ Chargement historique complet (fact_orders)
    8. 2_incremental_load_fact.ipynb    â†’ Chargement incrÃ©mental (exÃ©cution quotidienne)

ğŸ“ 2_dashboarding/
    9. ExÃ©cuter denormalise_table_query_fmcg.txt â†’ CrÃ©e vw_fact_orders_enriched
```

**5. CrÃ©er et planifier le Job Databricks**
```
Databricks > Jobs & Pipelines > Create Job
â†’ Ajouter les tÃ¢ches avec dÃ©pendances (dimensions en parallÃ¨le â†’ faits)
â†’ Configurer le trigger : Every Day at 21:00 UTC
â†’ Run now pour valider âœ…
```

**6. Connecter Genie & ouvrir le Dashboard**
```
â†’ Genie : lier Ã  fmcg.gold.vw_fact_orders_enriched
â†’ Dashboard : importer fmcg_dashboard depuis 2_dashboarding/
```

---

## ğŸ§  Ce que ce projet dÃ©montre

| CompÃ©tence | Mise en Å“uvre |
|---|---|
| **Data Lake Design** | Organisation S3 par domaine, nomenclature claire |
| **Spark & PySpark** | Lecture CSV distribuÃ©e, transformations DataFrames |
| **Architecture Medallion** | Bronze / Silver / Gold avec sÃ©paration des responsabilitÃ©s |
| **ModÃ©lisation dimensionnelle** | Star Schema : dim_* + fact_orders |
| **Chargement incrÃ©mental** | Mise Ã  jour des tables sans recalcul complet |
| **Orchestration** | Databricks Jobs avec graphe de dÃ©pendances |
| **Planification** | Trigger quotidien automatisÃ© |
| **IA conversationnelle** | Genie connectÃ© Ã  la Gold layer |
| **Data Visualization** | Dashboard KPIs interactif |
| **SÃ©curitÃ© cloud** | IAM Role + External Location sans exposition de clÃ©s |

---

## ğŸ‘¤ Auteur

**Francois Louis Marie NTONGA**  
*Data Engineer Junior*

ğŸ“§ [francoislouismarie.contact@gmail.com](mailto:francoislouismarie.contact@gmail.com)  
ğŸ”— [LinkedIn](www.linkedin.com/in/francois-louis-marie-ntonga-7b982329b)

---

<div align="center">

*"La donnÃ©e n'a de valeur que si elle est accessible, fiable et exploitable."*

**â­ Star ce projet s'il vous a inspirÃ© !**

</div>
